{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fffa5f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "161ef55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "llm = ChatOllama(model=\"llama3.2:1b\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d5d5a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of Italy is Rome.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 57, 'total_tokens': 64, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CAUkf1Sl6RyW2RLnbFbseJGPpqc67', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--8bf126d0-3684-4191-89dc-4cfbef50f0b4-0', usage_metadata={'input_tokens': 57, 'output_tokens': 7, 'total_tokens': 64, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_list = [\n",
    "    SystemMessage(content=\"You are a helpful assistant!\"),\n",
    "    HumanMessage(content=\"What is the capital of France?\"),\n",
    "    AIMessage(content=\"THe capital of France is Paris\"),\n",
    "    HumanMessage(content=\"What is THe capital of Germary\"),\n",
    "    AIMessage(content=\"THe capital of Germany is Berlin\"),\n",
    "    HumanMessage(content=\"What is ter capital of Italy\"),\n",
    "]\n",
    "\n",
    "llm.invoke(message_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d5fee359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='Your are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the capital of France', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "# template 를 사용한 방법\n",
    "# template 을 사용해야 LCEL 을 사용 할수 있다.\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Your are a helpful assistant\"),\n",
    "    (\"human\", \"What is the capital of {country}\")\n",
    "])\n",
    "\n",
    "chat_prompt = chat_prompt_template.invoke({\"country\": \"France\"})\n",
    "\n",
    "print(chat_prompt)\n",
    "ai_message = llm.invoke(chat_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6d4a7515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of South Korea is Seoul.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "answer = output_parser.invoke(llm.invoke(chat_prompt_template.invoke({\"country\": \"Korea\"})))\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7d0245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'capital': 'Paris',\n",
       " 'population': 67,\n",
       " 'language': 'French',\n",
       " 'currency': 'Euro'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class CountryDetail(BaseModel):\n",
    "    capital: str = Field(description=\"The capital of the country\")\n",
    "    population: int = Field(description=\"The population of the country\")\n",
    "    language: str = Field(description=\"The language of the country\")\n",
    "    currency: str = Field(description=\"The currency of the country\")\n",
    "\n",
    "\n",
    "structured_llm = llm.with_structured_output(CountryDetail)\n",
    "\n",
    "country_detail_prompt = PromptTemplate(\n",
    "    template=\"\"\"Give following information about {country}:\n",
    "        - Capital\n",
    "        - Population\n",
    "        - language\n",
    "        - Currency\n",
    "\n",
    "        return it in JSON format. and return the JSON dictionary only\n",
    "    \"\"\",\n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "\n",
    "json_ai_message = structured_llm.invoke(country_detail_prompt.invoke({\"country\": \"France\"}))\n",
    "# json 으로 변환\n",
    "json_ai_message.model_dump()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6ab9256d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Italy\n",
      "first={\n",
      "  country: PromptTemplate(input_variables=['information'], input_types={}, partial_variables={}, template='Guess teh name of the country based on the follwing information\\n    {information}\\n    return the name of the country only\\n\\n    ')\n",
      "           | ChatOllama(model='llama3.2:1b')\n",
      "           | StrOutputParser()\n",
      "} middle=[] last=ChatPromptTemplate(input_variables=['country'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Your are a helpful assistant'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['country'], input_types={}, partial_variables={}, template='What is the capital of {country}'), additional_kwargs={})])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='Your are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the capital of Italy.', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LCEL 체인\n",
    "country_prompt = PromptTemplate(\n",
    "    template=\"\"\"Guess teh name of the country based on the follwing information\n",
    "    {information}\n",
    "    return the name of the country only\n",
    "\n",
    "    \"\"\",\n",
    "    input_variables=[\"information\"],\n",
    ")\n",
    "\n",
    "country_chain = country_prompt | llm | output_parser\n",
    "answer1 = country_chain.invoke({\"information\": \"This country is very famous for its wine in Europe\"})\n",
    "\n",
    "print(answer1)\n",
    "\n",
    "final_chain = {\"country\": country_chain} | chat_prompt_template\n",
    "\n",
    "print(final_chain)\n",
    "\n",
    "final_chain.invoke({\"information\": \"This country is very famous for its wine in Europe\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af35ec0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e4cf16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecec016d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-application",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
